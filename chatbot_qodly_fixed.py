# chatbot_qodly_fixed.py
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from gemini_llm_simple import GeminiLLM  # Utilise la version simple
import gradio as gr
import os

# üîê Configuration
GEMINI_API_KEY = "AIzaSyBLtrLx8_z4YYykZEoA90uDqIN3K5K_2TU"  

# Template pour am√©liorer les r√©ponses
PROMPT_TEMPLATE = """
Tu es un assistant sp√©cialis√© dans la documentation. 
Utilise le contexte suivant pour r√©pondre √† la question de mani√®re pr√©cise et utile.

Contexte:
{context}

Question: {question}

Instructions:
- R√©ponds en fran√ßais
- Sois pr√©cis et concis et explique 
- Cite les sources quand c'est pertinent

R√©ponse:
"""

def initialize_chatbot():
    """Initialise le chatbot avec l'index FAISS et Gemini"""
    
    # V√©rifier si l'index existe
    if not os.path.exists("qodly_index"):
        return None, None, "‚ùå Index FAISS non trouv√©. Ex√©cutez d'abord build_faiss_index.py"
    
    # V√©rifier la cl√© API
    if not GEMINI_API_KEY or GEMINI_API_KEY == "VOTRE_CLE_API_GEMINI":
        return None, None, "‚ùå Cl√© API Gemini non configur√©e. Modifiez GEMINI_API_KEY dans le code."
    
    try:
        # Charger les embeddings et l'index FAISS
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
        vectorstore = FAISS.load_local(
            "qodly_index", 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        
        # Initialiser le mod√®le Gemini
        llm = GeminiLLM(api_key=GEMINI_API_KEY)
        
        # Cr√©er le retriever
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}  # R√©cup√©rer les 3 documents les plus pertinents
        )
        
        return retriever, llm, "‚úÖ Chatbot initialis√© avec succ√®s"
        
    except Exception as e:
        return None, None, f"‚ùå Erreur lors de l'initialisation: {str(e)}"

# Initialiser le chatbot
retriever, llm, init_message = initialize_chatbot()
print(init_message)

def repondre(question, historique):
    """Fonction pour r√©pondre aux questions"""
    
    if retriever is None or llm is None:
        return "", historique + [{"role": "user", "content": question}, {"role": "assistant", "content": "‚ùå Chatbot non initialis√©. V√©rifiez votre configuration."}]
    
    if not question.strip():
        return "", historique + [{"role": "user", "content": question}, {"role": "assistant", "content": "‚ùì Veuillez poser une question."}]
    
    try:
        # Rechercher des documents pertinents
        docs = retriever.get_relevant_documents(question)
        
        # Construire le contexte
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # Construire le prompt
        prompt = PROMPT_TEMPLATE.format(context=context, question=question)
        
        # G√©n√©rer la r√©ponse
        reponse = llm.invoke(prompt)
        
        # Extraire les sources
        sources = []
        for doc in docs:
            source = doc.metadata.get('source', 'inconnu')
            sources.append(f"üìÑ {os.path.basename(source)}")
        
        # Formater la r√©ponse compl√®te
        reponse_complete = reponse
        if sources:
            reponse_complete += f"\n\n**Sources:**\n" + "\n".join(set(sources))
        
        # Mettre √† jour l'historique au format messages
        historique_updated = historique + [
            {"role": "user", "content": question},
            {"role": "assistant", "content": reponse_complete}
        ]
        
        return "", historique_updated
        
    except Exception as e:
        error_msg = f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}"
        historique_updated = historique + [
            {"role": "user", "content": question},
            {"role": "assistant", "content": error_msg}
        ]
        return "", historique_updated

def clear_chat():
    """Efface l'historique du chat"""
    return [], ""

# Interface Gradio
with gr.Blocks(title="ü§ñ Chatbot Documentation", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ü§ñ Chatbot Documentation Qodly")
    gr.Markdown("Posez des questions sur votre documentation. R√©ponses g√©n√©r√©es avec Gemini.")
    
    chatbot = gr.Chatbot(
        label="Conversation",
        height=400,
        show_label=True,
        type="messages"
    )
    
    with gr.Row():
        msg = gr.Textbox(
            label="Votre question",
            placeholder="Posez votre question ici...",
            lines=2,
            scale=4
        )
        submit_btn = gr.Button("Envoyer", scale=1, variant="primary")
    
    with gr.Row():
        clear_btn = gr.Button("Effacer", scale=1)
    
    # √âv√©nements
    msg.submit(repondre, [msg, chatbot], [msg, chatbot])
    submit_btn.click(repondre, [msg, chatbot], [msg, chatbot])
    clear_btn.click(clear_chat, None, [chatbot, msg])

if __name__ == "__main__":
    # Pour Render : r√©cup√©rer le port depuis la variable d'environnement
    port = int(os.environ.get("PORT", 7860))  # 7860 en fallback local

    demo.launch(
        server_name="0.0.0.0",  # Obligatoire pour Render
        server_port=port,
        show_error=True
    )